---
title: "Coach Modeling"
author: "Peyton Lindogan"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readr)
#install.packages("Hmisc")
library(Hmisc)
#install.packages("Stat2Data")
library(Stat2Data)
library(dplyr)
#install.packages("car")
library(car)
#install.packages("leaps")
library(leaps)
#install.packages("tidymodels")
library(tidymodels)
library(caret)
library(psych)
library(glmnet)
#install.packages("corrplot")
library(corrplot)
library(car)
```

```{r, include = FALSE}
#read data and format to include only the variables of interest
ALL = read.csv("Coaches_Cleaning_ALL_History.csv")
ALL_Filter = filter(ALL, Coach_seasons_team ==1)
as.numeric(unlist(ALL_Filter$Coach_games_season))
as.numeric(unlist(ALL_Filter$Coach_games_season_win))
as.numeric(unlist(ALL_Filter$Coach_games_career_PO_HC_win))
as.numeric(unlist(ALL_Filter$Coach_games_career_PO_HC_loss))

ALL_Filter = ALL_Filter %>%
  mutate(Coach_games_season_win_pct = Coach_games_season_win/Coach_games_season, Coach_games_career_PO_win_pct = Coach_games_career_PO_HC_win/(Coach_games_career_PO_HC_win + Coach_games_career_PO_HC_loss))
ALL_Filter = ALL_Filter[-c(24,76,97),]
ALL_Clean = select(ALL_Filter, -c(1,2,3,4,6,7,8,9,10,11,13,14,15,16,17,21,22,25,26,27,28,30,32,43,44,45,47,48,49,50, 63:69))
ALL_Clean = select(ALL_Clean, -c(2,4,5,6,9,18,22,23,24,34))
ALL_Clean_test = select(ALL_Filter,-c(13:17,63:69,71))

```

MODELING BEGINS HERE
Start with every coach

```{r, include = FALSE}
ALL_Filter11 = ALL
as.numeric(unlist(ALL_Filter11$Coach_games_season))
as.numeric(unlist(ALL_Filter11$Coach_games_season_win))
as.numeric(unlist(ALL_Filter11$Coach_games_career_PO_HC_win))
as.numeric(unlist(ALL_Filter11$Coach_games_career_PO_HC_loss))


ALL_Filter11 = ALL_Filter11[-c(82,249,297,406),]
ALL_Clean11 = select(ALL_Filter11, -c(1,2,3,6,8:40, 63:69))

```

Try lasso

```{r}
set.seed(123)
#ALL_Clean11.5 = select(ALL_Clean11, -c(3))

#test and train split
split <- createDataPartition(y = ALL_Clean11$Coach_games_season_win, p = 0.8, list = FALSE)
train_data1 = ALL_Clean11[split, ]
test_data1 = ALL_Clean11[-split, ]
```
As the L1 Norm increases, variables "enter" the model as their coefficients take non-zero values.

```{r}
set.seed(123)
cv_lasso = cv.glmnet(x=as.matrix(subset(train_data1, select = -c(Coach_games_season_win))),y=as.vector(train_data1$Coach_games_season_win), alpha = 1)
best_lambda = cv_lasso$lambda.min
#lasso regularization path
plot(cv_lasso)

#retrieve coefficients
coefficients(cv_lasso, s=best_lambda)
```
Here, cross validation error is the same as MSE. Our best lambda is .2104. I think our model is really bad if the MSE is this high. Uses K = 10 for k fold. Even when using cross validation, you still need to do a test and training set! 
```{r}
#predictions
predictions = predict(cv_lasso, newx = as.matrix(subset(test_data1, select = -c(Coach_games_season_win))), s= best_lambda)
#mean squared error
MSE = mean((predictions - test_data1$Coach_games_season_win)^2)
MSE
#root mean squared error
RMSE = sqrt(MSE)
RMSE
#r squared
rsquared <- 1 - (sum((test_data1$Coach_games_season_win - predictions)^2) / sum((test_data1$Coach_games_season_win - mean(test_data1$Coach_games_season_win))^2))
rsquared
```
test to see if the residuals are correlated ( a key assumption of lasso regression is that they are NOT)

```{r}
y = rnorm(100)
residuals = test_data1$Coach_games_season_win - predictions
ggplot() +
  geom_point(aes(x = predictions, y = residuals)) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Lasso Residuals Plot", x = "Predicted Values", y = "Residuals")
```
residuals don't appear correlated yay! (idk if I should do it for the others too) (answer, I SHOULD)

```{r}
residuals2 = test_data2$Coach_games_season_win - predictions2
ggplot() +
  geom_point(aes(x = predictions2, y = residuals2)) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Lasso Residuals Plot", x = "Predicted Values", y = "Residuals")

residuals3 = test_data3$Coach_games_season_win - predictions3
ggplot() +
  geom_point(aes(x = predictions3, y = residuals3)) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Lasso Residuals Plot", x = "Predicted Values", y = "Residuals")
```



Now do all first year head coaches with a team

```{r, include = FALSE}
ALL_Filter12 = ALL
as.numeric(unlist(ALL_Filter12$Coach_games_season))
as.numeric(unlist(ALL_Filter12$Coach_games_season_win))
as.numeric(unlist(ALL_Filter12$Coach_games_career_PO_HC_win))
as.numeric(unlist(ALL_Filter12$Coach_games_career_PO_HC_loss))
ALL_Filter12 = filter(ALL_Filter12, Coach_seasons_team ==1)


ALL_Clean12 = select(ALL_Filter12, -c(1,2,3,6,8:40, 63:69))
ALL_Clean12 = ALL_Clean12[-c(24,76,97),]
```

Try lasso you fool

```{r}
set.seed(123)

#test and train split
split2 <- createDataPartition(y = ALL_Clean12$Coach_games_season_win, p = 0.8, list = FALSE)
train_data2 = ALL_Clean12[split2, ]
test_data2 = ALL_Clean12[-split2, ]
```

```{r}
set.seed(123)
cv_lasso2 = cv.glmnet(x=as.matrix(subset(train_data2, select = -c(Coach_games_season_win))),y=as.vector(train_data2$Coach_games_season_win), alpha = 1)
best_lambda2 = cv_lasso2$lambda.min
#lasso regularization path
plot(cv_lasso2)

#retrieve coefficients
coefficients(cv_lasso2, s=best_lambda2)
```

```{r}
#predictions
predictions2 = predict(cv_lasso2, newx = as.matrix(subset(test_data2, select = -c(Coach_games_season_win))), s= best_lambda2)
#mean squared error
MSE2 = mean((predictions2 - test_data2$Coach_games_season_win)^2)
MSE2
#root mean squared error
RMSE2 = sqrt(MSE2)
RMSE2
#r squared
rsquared2 <- 1 - (sum((test_data2$Coach_games_season_win - predictions2)^2) / sum((test_data2$Coach_games_season_win - mean(test_data2$Coach_games_season_win))^2))
rsquared2
```


Now do all first year head coaches with a team

```{r, include = FALSE}
ALL_Filter13 = ALL
as.numeric(unlist(ALL_Filter13$Coach_games_season))
as.numeric(unlist(ALL_Filter13$Coach_games_season_win))
as.numeric(unlist(ALL_Filter13$Coach_games_career_PO_HC_win))
as.numeric(unlist(ALL_Filter13$Coach_games_career_PO_HC_loss))
ALL_Filter13 = filter(ALL_Filter13, Coach_seasons_team ==1)
ALL_Filter13 = filter(ALL_Filter13, HC_seasons_total ==1)


ALL_Clean13 = select(ALL_Filter13, -c(1,2,3,6,8:40, 63:69))
ALL_Clean13 = ALL_Clean13[-c(9),]
```

Try lasso you fool

```{r}
set.seed(123)

#test and train split
split3 <- createDataPartition(y = ALL_Clean13$Coach_games_season_win, p = 0.7, list = FALSE)
train_data3 = ALL_Clean13[split3, ]
test_data3 = ALL_Clean13[-split3, ]
```

```{r}
set.seed(123)
cv_lasso3 = cv.glmnet(x=as.matrix(subset(train_data3, select = -c(Coach_games_season_win))),y=as.vector(train_data3$Coach_games_season_win), alpha = 1)
best_lambda3 = cv_lasso3$lambda.min
#lasso regularization path
plot(cv_lasso3)

#retrieve coefficients
coefficients(cv_lasso3, s=best_lambda3)
```


```{r}
#predictions
predictions3 = predict(cv_lasso3, newx = as.matrix(subset(test_data3, select = -c(Coach_games_season_win))), s= best_lambda3)
#mean squared error
MSE3 = mean((predictions3 - test_data3$Coach_games_season_win)^2)
MSE3
#root mean squared error
RMSE3 = sqrt(MSE3)
RMSE3
#r squared
rsquared3 <- 1 - (sum((test_data3$Coach_games_season_win - predictions3)^2) / sum((test_data3$Coach_games_season_win - mean(test_data3$Coach_games_season_win))^2))
rsquared3
```

Check for some autocorrelation stuff
```{r}
ALL_Filter11 = ALL_Filter11 %>%
  mutate(Coach_games_season_win_pct = Coach_games_season_win/Coach_games_season)
plot1 = ggplot() +
  geom_point(aes(x = ALL_Filter11$Coach_games_season_win_pct, y = ALL_Filter11$previous_season_win_pct))
cor(ALL_Filter11$Coach_games_season_win_pct, ALL_Filter11$previous_season_win_pct)
plot1 + labs(
  title = "Win percentage of a team from the previous year vs. win percentage of a team from the current year.",
  x = "Percentage of games won in the current season.",
  y = "Percentage of games won the previous season."
)
plot2 = ggplot() +
  geom_point(aes(x = ALL_Filter11$Coach_games_season_win, y = ALL_Filter11$HC_seasons_total))
plot2 + labs(
  title = "Games won in a season vs total number of years as a head coach.",
  x = "Number of games won in a season.",
  y = "Number of years as a head coach."
)
```
When looking at the relationship between previous season win percentage and current season win percentage, there is a noticeable correlation between the two. This fact violates the autocorrelation parameter, meaning that the observations of future data are likely to be correlated with past data.
Additionally, when looking at the difference in win percentage over the season, there's no clear trend, which may help to explain why a linear model is bad. Additionally, there's no noticeable difference in trend between percentage and actual games. Thus, I also checked to see if there was trend in different filter types dependent on whether the data was first year head coaches or not. Even then there's no discernible trend in the graphs.  

Now look to see if the data abides by stationarity.

```{r}
#checking if mean changes
means = ggplot() +
  geom_point(aes(x = ALL_Filter11$season, y = mean(ALL_Filter11$Coach_games_season_win)))
means + labs(
  title = "Mean games won in a season over time.",
  x = "mean of games won in a season.",
  y = "Seasons within the NBA."
)
#checking if variance changes
variance = ggplot() +
  geom_point(aes(x = ALL_Filter11$season, y = sd(ALL_Filter11$Coach_games_season_win)))
variance + labs(
  title = "Variance of games won in a season over time.",
  x = "Standard deviation of games won in a season.",
  y = "Seasons within the NBA."
)

```
Since the mean and variance remain the same across time, we can say that the condition of stationarity is met.
But I think you want to do the Durbin-Watson test to see if there is autocorrelation for sure. We can't do it on the regular lasso model because lasso shrinks coefficients leading to a sparse model. So I say we try on a basic model.

```{r}
lm_mid = lm(train_data1$Coach_games_season_win~., data = train_data1)
predictions_lm_mid = predict(lm_mid, newdata = test_data1)
residuals_mid = residuals(lm_mid, newdata = test_data1)
dw_test = durbinWatsonTest(lm_mid)
print(dw_test)

```
results are not good, there is likely autocolinearlity because the p value is less than .05 we reject the null hypothesis and conclude that the residuals in this regression model are autocorrleated. Now check VIF to see where the multicollinearity lies.

```{r}
vif(lm_mid)
```
Aliased coefficients are not good at all.

Check to see if there is correlation between our predictor variable and the remaining data.

```{r}
cor(ALL_Clean11$Coach_games_season_win, ALL_Clean11[-3])
cor_matrix = cor(train_data1)
cor_with_specific_va = cor_matrix[, "Coach_games_season_win"]
corrplot(cor_matrix, type = "upper", method = "color", tl.col = "black")
```
There is a lot of correlation present which is not good for determining independence.
